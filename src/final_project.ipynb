{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78e68906-89b6-4b44-a414-4d27920fbd01",
   "metadata": {},
   "source": [
    "Team Members\n",
    "- Yutian Wang(#85994168): Yutian contributed to data cleaning, formatting/standardization, visualization, time-series analysis using linear regression, and k-fold cross-validation.\n",
    "- Ronin Cunningham(#43949676): Ronin contributed to implementing the PyTorch neural network to further analyze the weather and corn prices.\n",
    "- Prayus Shrestha(#55823454): Prayus contributed to gathering the data through scraping and various APIs.\n",
    "- Ebin Tomy(#44912301): Ebin contributed by writing the formal analysis and explaining each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b74e02-73c8-4881-8c75-55f108764468",
   "metadata": {},
   "source": [
    "# Weather Impacts on US Corn Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b15ebe-b1dc-463e-a4ca-b9895ea941f6",
   "metadata": {},
   "source": [
    "TO-DO: opening paragraph.  This study focuses on the relationship between weather and corn prices in US..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b1e0d-065a-45b8-b2cc-c766fffdd534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from rnn_model import RNNModel\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from gru_model import GRUModel\n",
    "import shap\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3706245-d814-4814-8a7d-36313ccec02e",
   "metadata": {},
   "source": [
    "TO-DO: incorporate Prayus' data scrapping section here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c44046-14dd-416f-b19d-53b91d0cf74f",
   "metadata": {},
   "source": [
    "TO-DO: add description for these two data sets. source? meaning of each column? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8b3e2e-007f-402c-8183-9219e1c24234",
   "metadata": {},
   "source": [
    "### Data Cleaning and Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952929a-910e-4094-9d8f-8b6dd5921f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df = pd.read_csv('../data/weather_data.csv', index_col=0)\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c2f63-db91-454f-9a4c-12e35a12045a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_df.index = pd.to_datetime(weather_df[\"time\"])\n",
    "weather_df.index.name = \"date\"\n",
    "weather_df[\"date\"] = pd.to_datetime(weather_df.index)\n",
    "weather_df = weather_df[[\"temperature_2m_mean\", \"precipitation_sum\"]]\n",
    "weather_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d00ec3-10f1-4a02-a1dd-4cd1132cf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df = pd.read_csv(\"../data/crude_oil.csv\", index_col=0)\n",
    "lumber_df = pd.read_csv(\"../data/lumber_data.csv\", index_col=0)\n",
    "oat_df = pd.read_csv(\"../data/oat_data.csv\", index_col=0)\n",
    "wheat_df = pd.read_csv(\"../data/wheat_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99f5de-a506-49fb-89e8-15a520a4d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "corn_df = pd.read_csv(\"../data/corn_data.csv\", index_col = 0)\n",
    "corn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2a760a-139e-4204-af1c-3eb777241709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_commodity_df(df: pd.DataFrame, price_col_name: str = \"price\") -> pd.DataFrame: \n",
    "    \"\"\"Cleans the Commodity DataFrame\"\"\"\n",
    "    \n",
    "    # Setting the index to be the date\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.index.name = \"date\"\n",
    "    \n",
    "    # Setting price to be the day's midpoint, and dropping unnecessary columns\n",
    "    df[price_col_name] = (df[\"high\"] + df[\"low\"]) / 2 \n",
    "    df = df[[price_col_name]]\n",
    "    df = df.dropna() \n",
    "\n",
    "    # Sorting by date\n",
    "    df = df.sort_index()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915dffd-e31c-4e8e-9a79-b05db5d6d2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "corn_df = clean_commodity_df(df=corn_df, price_col_name=\"corn_price\")\n",
    "corn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e620c0e-09f2-4df1-85f3-b09478a935b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "oil_df = clean_commodity_df(df=oil_df, price_col_name=\"oil_price\")\n",
    "lumber_df = clean_commodity_df(df=lumber_df, price_col_name=\"lumber_price\")\n",
    "oat_df = clean_commodity_df(df=oat_df, price_col_name=\"oat_price\")\n",
    "wheat_df = clean_commodity_df(df=wheat_df, price_col_name=\"wheat_price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b174c-681c-432a-9645-5ae42bdef044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fa605-ee5c-480a-997c-9f36236edc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging features\n",
    "X = pd.concat([oil_df, lumber_df, oat_df, wheat_df, weather_df], axis=1).dropna()\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0e6691-3b68-455e-a00f-0ecaac51ae2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag variables \n",
    "lags = [1, 7, 30, 84, 365] # day, week, month, 84 days = time taken for corn to grow, year\n",
    "\n",
    "# Creating lags in df\n",
    "for lag in lags: \n",
    "    X[f\"temp_lag_{lag}\"] = X[\"temperature_2m_mean\"].shift(lag)\n",
    "    X[f\"precip_lag_{lag}\"] = X[\"precipitation_sum\"].shift(lag)\n",
    "\n",
    "X.dropna(inplace=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bfca77-bfb2-445c-8661-d98ad6e8eaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoregressive \n",
    "X = X.merge(corn_df, on=\"date\")\n",
    "X[\"AR1_corn_price\"] = X[\"corn_price\"].shift(1)\n",
    "X.drop(\"corn_price\", axis=1, inplace=True)\n",
    "X.dropna(inplace=True)\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a78644-afb0-48ab-b6b9-5a3b47ffea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X.join(corn_df, how=\"inner\")[\"corn_price\"]\n",
    "\n",
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "\n",
    "# Scaling inputs based on training data\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a62fa9-9796-454f-a834-37acdd4d61d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPRegressor((100, 100), random_state=123, max_iter=1000000)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "# mlp.predict(X_test)\n",
    "# mlp.predict(X_test)\n",
    "# mse_nn = mean_squared_error(y_test, mlp.predict(X_test))\n",
    "# print(mse_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e7053-ec0a-48aa-bbff-c672565591b1",
   "metadata": {},
   "source": [
    "It's important to consider the time lag between the weather and corn price data. We should account for the fact that weather conditions in a given year may affect the corn harvest and therefore corn prices mse_nnthe following year. So we shift the index of corn_data backwards by 365 days and create a new column called 'shifted_price_date' with the shifted index values, then to merge the two dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858746cd-4c8f-4ee6-9238-b324dfcdf84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_nn = mean_squared_error(y_test, mlp.predict(X_test_scaled))\n",
    "print(mse_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1546f20b-be05-4a2c-9d01-b19bd19cc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corn_data['shifted_price_date'] = corn_data.index - pd.DateOffset(years=1)\n",
    "# corn_data = corn_data.reset_index()\n",
    "# merged_data = pd.merge(corn_data, weather_data, left_on='shifted_price_date', right_on='weather_date')\n",
    "# merged_data = merged_data[['corn_price_date', 'avg_price', 'temperature_2m_mean', 'precipitation_sum']]\n",
    "# merged_data = merged_data.set_index('corn_price_date')\n",
    "# merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc79f683-f9e1-4843-9270-eca809ae53c7",
   "metadata": {},
   "source": [
    "### Data Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd66d7e-5f7d-4daa-9770-3719af045877",
   "metadata": {},
   "source": [
    "As temperature, precipitation, and price are on different scales, we standardize them to make it easier for comparison. Variables that have large values, such as temperature or precipitation, can dominate the analysis if they are not standardized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064c78e-99f6-491a-933e-5704e4dfe145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# merged_data_scaled = pd.DataFrame(scaler.fit_transform(merged_data), columns=merged_data.columns, index=merged_data.index)\n",
    "# merged_data_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce6722-8086-49c7-b963-0d1b4ef66d30",
   "metadata": {},
   "source": [
    "### Trial Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e288986-da64-4a53-88d0-e55ba09350e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the time-series\n",
    "# merged_data_scaled.plot(y='avg_price', figsize=(10,5))\n",
    "# plt.xlabel('Date')\n",
    "# plt.ylabel('Corn Average Price')\n",
    "# plt.title('Corn Average Price Overview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde6405f-7bfa-4986-b4ab-b7a117320091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(merged_data_scaled['temperature_2m_mean'], merged_data_scaled['avg_price'])\n",
    "# plt.xlabel('Temperature (°C)')\n",
    "# plt.ylabel('Avg. Corn Price ($/bushel)')\n",
    "# plt.title('Temperature vs. Avg. Corn Price')\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(merged_data_scaled['precipitation_sum'], merged_data_scaled['avg_price'])\n",
    "# plt.xlabel('Precipitation (mm)')\n",
    "# plt.ylabel('Avg. Corn Price ($/bushel)')\n",
    "# plt.title('Precipitation vs. Avg. Corn Price')\n",
    "# plt.show()\n",
    "\n",
    "# plt.scatter(merged_data_scaled['temperature_2m_mean'], merged_data_scaled['precipitation_sum'])\n",
    "# plt.xlabel('Temperature (°C)')\n",
    "# plt.ylabel('Precipitation (mm)')\n",
    "# plt.title('Temperature vs. Precipitation')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de6c13d-1a8c-4bc4-b4c1-b09537244bfd",
   "metadata": {},
   "source": [
    "Based on the plots, there doesn't seem to be a strong correlation between temperature and precipitation with corn prices. It's also possible that the relationship between weather and corn prices is more complex than a simple linear relationship, and may require further analysis to uncover."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa454ef0-4a6e-44bd-ac2a-9915f1085fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corr_matrix = merged_data_scaled[['avg_price', 'temperature_2m_mean', 'precipitation_sum']].corr()\n",
    "# print(corr_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b60e337-3e66-4bc7-9a88-c345499c8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(corr_matrix, cmap='coolwarm', annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c48602-d359-4fd4-b5e4-4be6f7f3ba1b",
   "metadata": {},
   "source": [
    "### Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9801cf5-790d-4ff3-9111-3862bcb86cc4",
   "metadata": {},
   "source": [
    "Using lags of 30 days and 60 days to capture any possible autocorrelation, without overfitting the model or introducing too much noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf65e4d-4a6c-4057-8d51-6a90cc3426a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create lagged variables\n",
    "# merged_data_scaled['temp_lag1'] = merged_data_scaled['temperature_2m_mean'].shift(30)\n",
    "# merged_data_scaled['temp_lag2'] = merged_data_scaled['temperature_2m_mean'].shift(60)\n",
    "# merged_data_scaled['precip_lag1'] = merged_data_scaled['precipitation_sum'].shift(30)\n",
    "# merged_data_scaled['precip_lag2'] = merged_data_scaled['precipitation_sum'].shift(60)\n",
    "# merged_data_scaled = merged_data_scaled.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daca781a-382d-4a39-a714-de68a1614ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot the merged data\n",
    "# fig, axs = plt.subplots(4, figsize=(12, 12))\n",
    "# axs[0].plot(merged_data_scaled.index, merged_data_scaled['avg_price'], label='Corn Price')\n",
    "# axs[0].legend()\n",
    "# axs[1].plot(merged_data_scaled.index, merged_data_scaled['temperature_2m_mean'], label='Temperature')\n",
    "# axs[1].legend()\n",
    "# axs[2].plot(merged_data_scaled.index, merged_data_scaled['precipitation_sum'], label='Precipitation')\n",
    "# axs[2].legend()\n",
    "# axs[3].plot(merged_data_scaled.index, merged_data_scaled['temp_lag1'], label='Temperature Lag 1')\n",
    "# axs[3].plot(merged_data_scaled.index, merged_data_scaled['temp_lag2'], label='Temperature Lag 2')\n",
    "# axs[3].plot(merged_data_scaled.index, merged_data_scaled['precip_lag1'], label='Precipitation Lag 1')\n",
    "# axs[3].plot(merged_data_scaled.index, merged_data_scaled['precip_lag2'], label='Precipitation Lag 2')\n",
    "# axs[3].legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd58ef4-c5c4-4df4-b19c-ccab455ad885",
   "metadata": {},
   "source": [
    "### Linear Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d784476f-f9ab-4d1e-b22e-e579ed9bf633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = merged_data_scaled[['temp_lag1', 'temp_lag2', 'precip_lag1', 'precip_lag2']]\n",
    "# y = merged_data_scaled['avg_price']\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# lr_model = LinearRegression()\n",
    "# lr_model.fit(X_train, y_train)\n",
    "\n",
    "# print('Coefficients:', lr_model.coef_)\n",
    "# print('Intercept:', lr_model.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ed13c-64fe-432e-b318-e5b93f2a134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make predictions on test data\n",
    "# y_pred = lr_model.predict(X_test)\n",
    "\n",
    "# train_rmse = np.sqrt(np.mean((lr_model.predict(X_train) - y_train) ** 2))\n",
    "# test_rmse = np.sqrt(np.mean((y_pred - y_test) ** 2))\n",
    "\n",
    "# print(f'Train RMSE: {train_rmse}')\n",
    "# print(f'Test RMSE: {test_rmse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554a8558-31a9-4ec0-9a85-c363497cf83f",
   "metadata": {},
   "source": [
    "TO-DO: add some discussions here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e1d4c-712d-4d76-a3cb-0383d198682e",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f817430-e3aa-471a-86d5-1ac6c5f60b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr2 = LinearRegression()\n",
    "# X = merged_data_scaled.drop('avg_price', axis=1)\n",
    "# y = merged_data_scaled['avg_price']\n",
    "\n",
    "# mse_scores = -cross_val_score(lr2, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# print(f\"Average MSE: {mse_scores.mean():.4f}, Standard deviation: {mse_scores.std():.4f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a58bc0f",
   "metadata": {},
   "source": [
    "### RNN Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c695a504",
   "metadata": {},
   "source": [
    "TODO: explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275cdad-6b99-4822-ad6e-77387cf3ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_scaled, columns=X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc3ad71",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b36bf155",
   "metadata": {},
   "source": [
    "#### Basic RNN model with Adam optimizer and L2 regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fbd0a9a2",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0f06d0d",
   "metadata": {},
   "source": [
    "First, we need to set hyperparameters for our RNN model. After experimenting with various different configurations, below are the hyperparameters that I found to be best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a2020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "input_size = 17\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "sequence_length = 1\n",
    "\n",
    "# Prepare the train dataset\n",
    "X_train_arr = np.array(X_train_scaled)\n",
    "y_train_arr = np.array(y_train)\n",
    "\n",
    "X_train_arr = X_train_arr.reshape(-1, sequence_length, input_size)\n",
    "y_train_arr = y_train_arr.reshape(-1, sequence_length, output_size)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_arr).float(), torch.tensor(y_train_arr).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "rnn_model_instance = RNNModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(rnn_model_instance.parameters(), lr=learning_rate, weight_decay=1e-5)  # Add weight_decay for L2 regularization\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = rnn_model_instance(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "500b539c",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eaa364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test dataset\n",
    "X_test_arr = np.array(X_test_scaled)\n",
    "y_test_arr = np.array(y_test)\n",
    "\n",
    "X_test_arr = X_test_arr.reshape(-1, sequence_length, input_size)\n",
    "y_test_arr = y_test_arr.reshape(-1, sequence_length, output_size)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(X_test_arr).float(), torch.tensor(y_test_arr).float())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "rnn_model_instance.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = rnn_model_instance(inputs)\n",
    "        predictions.extend(outputs.numpy().flatten())\n",
    "        actuals.extend(targets.numpy().flatten())\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae_rnn = mean_absolute_error(actuals, predictions)\n",
    "mse_rnn = mean_squared_error(actuals, predictions)\n",
    "rmse_rnn = np.sqrt(mse_rnn)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae_rnn:.4f}')\n",
    "print(f'Mean Squared Error: {mse_rnn:.4f}')\n",
    "print(f'Root Mean Squared Error: {rmse_rnn:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27afc4f5",
   "metadata": {},
   "source": [
    "#### GRU model with Adam optimizer and L2 Regularization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7995b72f",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4340f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "\n",
    "input_size = 17\n",
    "hidden_size = 64\n",
    "output_size = 1\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "sequence_length = 1\n",
    "\n",
    "# Prepare the train dataset\n",
    "X_train_arr = np.array(X_train_scaled)\n",
    "y_train_arr = np.array(y_train)\n",
    "\n",
    "X_train_arr = X_train_arr.reshape(-1, sequence_length, input_size)\n",
    "y_train_arr = y_train_arr.reshape(-1, sequence_length, output_size)\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train_arr).float(), torch.tensor(y_train_arr).float())\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "gru_model_instance = GRUModel(input_size, hidden_size, output_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(gru_model_instance.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# Train the modelt\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = gru_model_instance(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5415d4d",
   "metadata": {},
   "source": [
    "##### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea93774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test dataset\n",
    "X_test_arr = np.array(X_test_scaled)\n",
    "y_test_arr = np.array(y_test)\n",
    "\n",
    "X_test_arr = X_test_arr.reshape(-1, sequence_length, input_size)\n",
    "y_test_arr = y_test_arr.reshape(-1, sequence_length, output_size)\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(X_test_arr).float(), torch.tensor(y_test_arr).float())\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Evaluate the model\n",
    "gru_model_instance.eval()\n",
    "predictions = []\n",
    "actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets in test_loader:\n",
    "        outputs = gru_model_instance(inputs)\n",
    "        predictions.extend(outputs.numpy().flatten())\n",
    "        actuals.extend(targets.numpy().flatten())\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "mae_gru = mean_absolute_error(actuals, predictions)\n",
    "mse_gru = mean_squared_error(actuals, predictions)\n",
    "rmse_gru = np.sqrt(mse_gru)\n",
    "\n",
    "print(f'Mean Absolute Error: {mae_gru:.4f}')\n",
    "print(f'Mean Squared Error: {mse_gru:.4f}')\n",
    "print(f'Root Mean Squared Error: {rmse_gru:.4f}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf2382db",
   "metadata": {},
   "source": [
    "### Shapley Values of our Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87bceea",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rnn_model_instance if rmse_rnn <= rmse_gru else gru_model_instance\n",
    "\n",
    "def model_wrapper(x):\n",
    "    x_tensor = torch.FloatTensor(x).unsqueeze(1)\n",
    "    with torch.no_grad():\n",
    "        output = best_model(x_tensor)\n",
    "    return output.numpy()\n",
    "\n",
    "# Initialize the explainer\n",
    "explainer = shap.Explainer(model_wrapper, X_train_scaled)\n",
    "\n",
    "# Compute SHAP values for X_test\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Plot SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
